{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook to duplicate ARGOG using cells \n",
    "## code is from main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade llama-index\n",
    "# !pip show llama-index\n",
    "\n",
    "# !pip install llama-index-agent-openai  llama-index-cli  llama-index-core  llama-index-embeddings-openai  llama-index-indices-managed-llama-cloud  llama-index-legacy  llama-index-llms-openai  llama-index-multi-modal-llms-openai  llama-index-program-openai  llama-index-question-gen-openai  llama-index-readers-file  llama-index-readers-llama-parse  nltk\n",
    "# !pip install -U llama-index\n",
    "# %pip install -q cohere llama-index-postprocessor-cohere-rerank\n",
    "# !pip install tonic-validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing stuff\n",
    "import openai\n",
    "import chromadb\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from utils import run_experiment, load_config\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import VectorStoreIndex, PromptTemplate, ServiceContext, Settings\n",
    "# from llama_index.settings import Settings\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
    "from llama_index.core.indices.query.query_transform import HyDEQueryTransform\n",
    "from llama_index.core.query_engine import TransformQueryEngine, RetrieverQueryEngine\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core.postprocessor import LLMRerank\n",
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "from tonic_validate import ValidateScorer, ValidateApi\n",
    "from tonic_validate.metrics import RetrievalPrecisionMetric, AnswerSimilarityMetric\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sample code to use tonic validate\n",
    "# from tonic_validate import Benchmark, ValidateApi, ValidateScorer\n",
    "\n",
    "# # Function to simulate getting a response and context from your LLM\n",
    "# # Replace this with your actual function call\n",
    "# def get_rag_response(question):\n",
    "#     return {\n",
    "#         \"llm_answer\": \"Paris\",\n",
    "#         \"llm_context_list\": [\"Paris is the capital of France.\"]\n",
    "#     }\n",
    "\n",
    "# benchmark = Benchmark(questions=[\"What is the capital of France?\"], answers=[\"Paris\"])\n",
    "# # Score the responses for each question and answer pair\n",
    "# scorer = ValidateScorer()\n",
    "# run = scorer.score(benchmark, get_rag_response)\n",
    "# validate_api = ValidateApi(\"opycxzKWmtHosC25-r_Kd31EbfLHVBerNEBj675AeS4\")\n",
    "# validate_api.upload_run(\"f6cd8c02-2ef3-42ac-bc69-6c2fd5e3df03\", run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SETUP --------------------------------------------------------------------------------------------------------------\n",
    "# Load the config file (.env vars)\n",
    "load_config()\n",
    "load_dotenv(override=True)\n",
    "# Set the OpenAI API key for authentication.\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "tonic_validate_api_key = os.getenv(\"HASSAN_TONIC_VALIDATE_API1\")\n",
    "tonic_validate_project_key = os.getenv(\"HASSAN_TONIC_VALIDATE_PROJECT1\")\n",
    "tonic_validate_benchmark_key = os.getenv(\"HASSAN_TONIC_VALIDATE_BENCHMARK1\")\n",
    "validate_api = ValidateApi(tonic_validate_api_key)\n",
    "# print (f'{tonic_validate_api_key} {tonic_validate_project_key} {tonic_validate_benchmark_key}')\n",
    "cohere_api_key = os.getenv(\"HASSAN_COHERE_DEFAULT_API\")\n",
    "# print (f'{tonic_validate_api_key} {tonic_validate_project_key} {tonic_validate_benchmark_key}')\n",
    "# cohere_api_key\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tonic_validate_project_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config into dictionary\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Create a config dictionary\n",
    "config = {\n",
    "    # 'cohere_api_key': os.getenv('COHERE_API_KEY'),\n",
    "    # 'another_setting': os.getenv('ANOTHER_SETTING'),\n",
    "    'tonic_validate_api_key': os.getenv(\"HASSAN_TONIC_VALIDATE_API1\"),\n",
    "    'tonic_validate_project_key':  os.getenv(\"HASSAN_TONIC_VALIDATE_PROJECT1\"),\n",
    "    'tonic_validate_benchmark_key':  os.getenv(\"HASSAN_TONIC_VALIDATE_BENCHMARK1\"),\n",
    "    'cohere_api_key':  os.getenv(\"HASSAN_COHERE_DEFAULT_API\")\n",
    "}\n",
    "\n",
    "# Print to verify\n",
    "print(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Service context\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\")\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\")\n",
    "# service_context = ServiceContext.from_defaults(llm = llm, embed_model = embed_model)\n",
    "# settings = Settings(llm=llm, embed_model=embed_model)\n",
    "Settings.llm = llm \n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # existing_collections = chroma_client.list_collections()\n",
    "# # print(existing_collections)\n",
    "# # First, check if collection exists and create it if it doesn't\n",
    "# try:\n",
    "#     chroma_collection = chroma_client.get_collection(\"ai_arxiv_full\")\n",
    "# except:\n",
    "#     # Create new collection\n",
    "#     chroma_collection = chroma_client.create_collection(\n",
    "#         name=\"ai_arxiv_full\",\n",
    "#         metadata={\"description\": \"ArXiv AI papers collection\"}\n",
    "#     )\n",
    "\n",
    "# # Now you can use the collection\n",
    "# vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "# index = VectorStoreIndex.from_vector_store(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.core import Settings, StorageContext, VectorStoreIndex\n",
    "# from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "# from llama_index.core.node_parser import SentenceWindowNodeParser  # Updated import path\n",
    "\n",
    "# # Create the collection first\n",
    "# try:\n",
    "#     chroma_collection_sentence_window = chroma_client.get_collection(\"ai_arxiv_sentence_window\")\n",
    "# except:\n",
    "#     chroma_collection_sentence_window = chroma_client.create_collection(\n",
    "#         name=\"ai_arxiv_sentence_window\",\n",
    "#         metadata={\"description\": \"ArXiv AI papers with sentence window processing\"}\n",
    "#     )\n",
    "\n",
    "# # Set up sentence window processing\n",
    "# node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "#     window_size=3,\n",
    "#     window_metadata_key=\"window\",\n",
    "#     original_text_metadata_key=\"original_text\"\n",
    "# )\n",
    "\n",
    "# # Update settings with the node parser\n",
    "# Settings.node_parser = node_parser\n",
    "\n",
    "# # Set up vector store and index\n",
    "# vector_store_sentence_window = ChromaVectorStore(\n",
    "#     chroma_collection=chroma_collection_sentence_window\n",
    "# )\n",
    "# storage_context = StorageContext.from_defaults(\n",
    "#     vector_store=vector_store_sentence_window\n",
    "# )\n",
    "\n",
    "# # Create the index\n",
    "# index_sentence_window = VectorStoreIndex.from_vector_store(\n",
    "#     vector_store=vector_store_sentence_window\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traditional VDB\n",
    "chroma_collection = chroma_client.get_collection(\"ai_arxiv_full\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence window VDB\n",
    "chroma_collection_sentence_window = chroma_client.get_collection(\"ai_arxiv_sentence_window\")\n",
    "vector_store_sentence_window = ChromaVectorStore(chroma_collection=chroma_collection_sentence_window)\n",
    "index_sentence_window = VectorStoreIndex.from_vector_store(vector_store=vector_store_sentence_window)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document summary VDB\n",
    "from llama_index.core import load_index_from_storage\n",
    "from llama_index.core import StorageContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storage_context = StorageContext.from_defaults(persist_dir=\"Obelix\")\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"ai_arxiv_doc_summary\")\n",
    "doc_summary_index = load_index_from_storage(llm=llm,\n",
    "                                              storage_context=storage_context,\n",
    "                                             embed_model = embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template\n",
    "with open(\"resources/text_qa_template.txt\", 'r', encoding='utf-8') as file:\n",
    "    text_qa_template_str = file.read()\n",
    "\n",
    "text_qa_template = PromptTemplate(text_qa_template_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tonic Validate setup\n",
    "benchmark = validate_api.get_benchmark(tonic_validate_benchmark_key)\n",
    "scorer = ValidateScorer(metrics=[RetrievalPrecisionMetric(), AnswerSimilarityMetric()],\n",
    "                        model_evaluator=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define query engines -------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive RAG\n",
    "query_engine_naive = index.as_query_engine(llm = llm,\n",
    "                                           text_qa_template=text_qa_template,\n",
    "                                           similarity_top_k=3,\n",
    "                                           embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cohere Rerank\n",
    "cohere_rerank = CohereRerank(api_key=config['cohere_api_key'], top_n=3)  # Ensure top_n matches k in naive RAG for comparability\n",
    "query_engine_rerank = index.as_query_engine(\n",
    "    similarity_top_k=10,\n",
    "    text_qa_template=text_qa_template,\n",
    "    node_postprocessors=[cohere_rerank],\n",
    "    llm = llm,\n",
    "    embed_model=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyDE\n",
    "hyde = HyDEQueryTransform(include_original=True)\n",
    "query_engine_hyde = TransformQueryEngine(query_engine_naive, hyde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyDE + Cohere Rerank\n",
    "query_engine_hyde_rerank = TransformQueryEngine(query_engine_rerank, hyde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximal Marginal Relevance (MMR)\n",
    "query_engine_mmr = index.as_query_engine(vector_store_query_mode=\"mmr\",\n",
    "                                         similarity_top_k=3,\n",
    "                                          embed_model=embed_model,\n",
    "                                          llm=llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Query\n",
    "vector_retriever = index.as_retriever(similarity_top_k=3)\n",
    "retriever_multi_query = QueryFusionRetriever(\n",
    "    [vector_retriever],\n",
    "    similarity_top_k=3,\n",
    "    num_queries=5,\n",
    "    llm=llm,\n",
    "    mode=\"reciprocal_rerank\",\n",
    "    use_async=False,\n",
    "    verbose=True\n",
    ")\n",
    "query_engine_multi_query = RetrieverQueryEngine.from_args(retriever_multi_query,\n",
    "                                                          verbose=True,\n",
    "                                                          embed_model=embed_model,\n",
    "                                                          llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Query + Cohere rerank + simple fusion\n",
    "retriever_multi_query_rerank = QueryFusionRetriever(\n",
    "    [vector_retriever],\n",
    "    similarity_top_k=10,\n",
    "    llm=llm,\n",
    "    num_queries=5,\n",
    "    mode=\"simple\",\n",
    "    use_async=False,\n",
    "    verbose=True\n",
    ")\n",
    "query_engine_multi_query_rerank = RetrieverQueryEngine.from_args(retriever_multi_query_rerank,\n",
    "                                                                 verbose=True,\n",
    "                                                                 node_postprocessors=[cohere_rerank],\n",
    "                                                                 embed_model=embed_model,\n",
    "                                                                 llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LLM Rerank\n",
    "llm_rerank = LLMRerank(choice_batch_size=10, top_n=3)\n",
    "query_engine_llm_rerank = index.as_query_engine(\n",
    "    similarity_top_k=10,\n",
    "    text_qa_template=text_qa_template,\n",
    "    node_postprocessors=[llm_rerank],\n",
    "    embed_model=embed_model,\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyDE + LLM Rerank\n",
    "query_engine_hyde_llm_rerank = TransformQueryEngine(query_engine_llm_rerank, hyde)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence window retrieval\n",
    "query_engine_sentence_window = index_sentence_window.as_query_engine(text_qa_template=text_qa_template,\n",
    "                                                                     similarity_top_k=3,\n",
    "                                                                      embed_model=embed_model,\n",
    "                                                                      llm=llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence window retrieval + Cohere rerank\n",
    "query_engine_sentence_window_rerank = index_sentence_window.as_query_engine(\n",
    "    similarity_top_k=10,\n",
    "    text_qa_template=text_qa_template,\n",
    "    node_postprocessors=[cohere_rerank],\n",
    "    embed_model=embed_model,\n",
    "    llm=llm\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence window retrieval + LLM Rerank\n",
    "query_engine_sentence_window_llm_rerank = index_sentence_window.as_query_engine(\n",
    "    similarity_top_k=10,\n",
    "    text_qa_template=text_qa_template,\n",
    "    node_postprocessors=[llm_rerank],\n",
    "    embed_model=embed_model,\n",
    "    llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence window retrieval + HyDE\n",
    "query_engine_sentence_window_hyde = TransformQueryEngine(query_engine_sentence_window, hyde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence window retrieval + HyDE + Cohere Rerank\n",
    "query_engine_sentence_window_hyde_rerank = TransformQueryEngine(query_engine_sentence_window_rerank, hyde)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence window retrieval + HyDE + LLM Rerank\n",
    "query_engine_sentence_window_hyde_llm_rerank = TransformQueryEngine(query_engine_sentence_window_llm_rerank, hyde)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document summary index + Cohere Rerank\n",
    "query_engine_doc_summary_rerank = doc_summary_index.as_query_engine(\n",
    "    similarity_top_k=5,\n",
    "    text_qa_template=text_qa_template,\n",
    "    node_postprocessors=[cohere_rerank],\n",
    "    llm=llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document summary index + HyDE + Cohere Rerank\n",
    "query_engine_hyde_doc_summary_rerank = TransformQueryEngine(query_engine_doc_summary_rerank, hyde)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run experiments -------------------------------------------------------------------------------------------------------\n",
    "# Dictionary of experiments, now referencing the predefined query engine objects\n",
    "experiments = {\n",
    "    # \"Classic VDB + Naive RAG\": query_engine_naive,\n",
    "    # \"Classic VDB + Cohere Rerank\": query_engine_rerank,\n",
    "    # \"Classic VDB + LLM Rerank\": query_engine_llm_rerank,\n",
    "    # \"Classic VDB + HyDE\": query_engine_hyde,\n",
    "    # \"Classic VDB + HyDE + Cohere Rerank\": query_engine_hyde_rerank,\n",
    "    # \"Classic VDB + HyDE + LLM Rerank\": query_engine_hyde_llm_rerank,\n",
    "    # \"Classic VDB + Maximal Marginal Relevance (MMR)\": query_engine_mmr,\n",
    "    # \"Classic VDB + Multi Query + Reciprocal\": query_engine_multi_query,\n",
    "    # \"Classic VDB + Multi Query + Cohere rerank\": query_engine_multi_query_rerank,\n",
    "    # \"Sentence window retrieval\": query_engine_sentence_window,\n",
    "    # \"Sentence window retrieval + Cohere rerank\": query_engine_sentence_window_rerank,\n",
    "    # \"Sentence window retrieval + LLM Rerank\": query_engine_sentence_window_llm_rerank,\n",
    "    # \"Sentence window retrieval + HyDE\": query_engine_sentence_window_hyde,\n",
    "    # \"Sentence window retrieval + HyDE + Cohere Rerank\": query_engine_sentence_window_hyde_rerank,\n",
    "    \"Sentence window retrieval + HyDE + LLM Rerank\": query_engine_sentence_window_hyde_llm_rerank,\n",
    "    # \"Document summary index + Cohere Rerank\": query_engine_doc_summary_rerank,\n",
    "    # \"Document summary index + HyDE + Cohere Rerank\": query_engine_hyde_doc_summary_rerank\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty DataFrame to collect results from all experiments\n",
    "all_experiments_results_df = pd.DataFrame(columns=['Run', 'Experiment', 'OverallScores'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving responses: 0it [00:00, ?it/s]\n",
      "Scoring responses: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence window retrieval + HyDE + LLM Rerank Run 1 Overall Scores: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving responses: 0it [00:00, ?it/s]\n",
      "Scoring responses: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence window retrieval + HyDE + LLM Rerank Run 2 Overall Scores: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving responses: 0it [00:00, ?it/s]\n",
      "Scoring responses: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence window retrieval + HyDE + LLM Rerank Run 3 Overall Scores: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving responses: 0it [00:00, ?it/s]\n",
      "Scoring responses: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence window retrieval + HyDE + LLM Rerank Run 4 Overall Scores: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving responses: 0it [00:00, ?it/s]\n",
      "Scoring responses: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence window retrieval + HyDE + LLM Rerank Run 5 Overall Scores: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving responses: 0it [00:00, ?it/s]\n",
      "Scoring responses: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence window retrieval + HyDE + LLM Rerank Run 6 Overall Scores: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving responses: 0it [00:00, ?it/s]\n",
      "Scoring responses: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence window retrieval + HyDE + LLM Rerank Run 7 Overall Scores: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving responses: 0it [00:00, ?it/s]\n",
      "Scoring responses: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence window retrieval + HyDE + LLM Rerank Run 8 Overall Scores: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving responses: 0it [00:00, ?it/s]\n",
      "Scoring responses: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence window retrieval + HyDE + LLM Rerank Run 9 Overall Scores: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving responses: 0it [00:00, ?it/s]\n",
      "Scoring responses: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence window retrieval + HyDE + LLM Rerank Run 10 Overall Scores: {}\n"
     ]
    }
   ],
   "source": [
    "# Loop through each experiment configuration, run it, and collect results\n",
    "for experiment_name, query_engine in experiments.items():\n",
    "    experiment_results_df = run_experiment(experiment_name,\n",
    "                                            query_engine,\n",
    "                                            scorer,\n",
    "                                            benchmark,\n",
    "                                            validate_api,\n",
    "                                            config['tonic_validate_project_key'],\n",
    "                                            upload_results=True,\n",
    "                                            runs=10)  # Adjust the number of runs as needed\n",
    "\n",
    "    # Append the results of this experiment to the master DataFrame\n",
    "    all_experiments_results_df = pd.concat([all_experiments_results_df, experiment_results_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'retrieval_precision'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Assuming all_experiments_results_df is your DataFrame\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m all_experiments_results_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRetrievalPrecision\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mall_experiments_results_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOverallScores\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mretrieval_precision\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1173\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1174\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1175\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1176\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1177\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1178\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1181\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1182\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\_libs\\lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[47], line 2\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Assuming all_experiments_results_df is your DataFrame\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m all_experiments_results_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRetrievalPrecision\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m all_experiments_results_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOverallScores\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mretrieval_precision\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'retrieval_precision'"
     ]
    }
   ],
   "source": [
    "# Assuming all_experiments_results_df is your DataFrame\n",
    "all_experiments_results_df['RetrievalPrecision'] = all_experiments_results_df['OverallScores'].apply(lambda x: x['retrieval_precision'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'RetrievalPrecision'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'RetrievalPrecision'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m levene\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Test for equal variances\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m stat, p \u001b[38;5;241m=\u001b[39m \u001b[43mlevene\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mall_experiments_results_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mall_experiments_results_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mExperiment\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mexp\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRetrievalPrecision\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexperiments\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLevene’s test: Statistics=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstat\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, p=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m>\u001b[39m alpha:\n",
      "Cell \u001b[1;32mIn[45], line 4\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m levene\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Test for equal variances\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m stat, p \u001b[38;5;241m=\u001b[39m levene(\u001b[38;5;241m*\u001b[39m(\u001b[43mall_experiments_results_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mall_experiments_results_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mExperiment\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mexp\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRetrievalPrecision\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m exp \u001b[38;5;129;01min\u001b[39;00m experiments\u001b[38;5;241m.\u001b[39mkeys()))\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLevene’s test: Statistics=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstat\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, p=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m>\u001b[39m alpha:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'RetrievalPrecision'"
     ]
    }
   ],
   "source": [
    "from scipy.stats import levene\n",
    "\n",
    "# Test for equal variances\n",
    "stat, p = levene(*(all_experiments_results_df[all_experiments_results_df['Experiment'] == exp]['RetrievalPrecision'] for exp in experiments.keys()))\n",
    "print(f\"Levene’s test: Statistics={stat}, p={p}\")\n",
    "if p > alpha:\n",
    "    print('Equal variances across groups (fail to reject H0)')\n",
    "else:\n",
    "    print('Unequal variances across groups (reject H0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Sentence window retrieval + HyDE + LLM Rerank'])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ARAGOG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
