{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook to duplicate ARGOG using cells \n",
    "## code is from main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade llama-index\n",
    "# !pip show llama-index\n",
    "\n",
    "# !pip install llama-index-agent-openai  llama-index-cli  llama-index-core  llama-index-embeddings-openai  llama-index-indices-managed-llama-cloud  llama-index-legacy  llama-index-llms-openai  llama-index-multi-modal-llms-openai  llama-index-program-openai  llama-index-question-gen-openai  llama-index-readers-file  llama-index-readers-llama-parse  nltk\n",
    "# !pip install -U llama-index\n",
    "# %pip install -q cohere llama-index-postprocessor-cohere-rerank\n",
    "# !pip install tonic-validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing stuff\n",
    "import openai\n",
    "import chromadb\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from utils import run_experiment, load_config\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import VectorStoreIndex, PromptTemplate, ServiceContext, Settings\n",
    "# from llama_index.settings import Settings\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
    "from llama_index.core.indices.query.query_transform import HyDEQueryTransform\n",
    "from llama_index.core.query_engine import TransformQueryEngine, RetrieverQueryEngine\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core.postprocessor import LLMRerank\n",
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "from tonic_validate import ValidateScorer, ValidateApi\n",
    "from tonic_validate.metrics import RetrievalPrecisionMetric, AnswerSimilarityMetric\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SETUP --------------------------------------------------------------------------------------------------------------\n",
    "# Load the config file (.env vars)\n",
    "load_config()\n",
    "load_dotenv(override=True)\n",
    "# Set the OpenAI API key for authentication.\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "tonic_validate_api_key = os.getenv(\"HASSAN_TONIC_VALIDATE_API1\")\n",
    "tonic_validate_project_key = os.getenv(\"HASSAN_TONIC_VALIDATE_PROJECT1\")\n",
    "tonic_validate_benchmark_key = os.getenv(\"HASSAN_TONIC_VALIDATE_BENCHMARK1\")\n",
    "validate_api = ValidateApi(tonic_validate_api_key)\n",
    "# print (f'{tonic_validate_api_key} {tonic_validate_project_key} {tonic_validate_benchmark_key}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Service context\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\")\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\")\n",
    "# service_context = ServiceContext.from_defaults(llm = llm, embed_model = embed_model)\n",
    "# settings = Settings(llm=llm, embed_model=embed_model)\n",
    "Settings.llm = llm \n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traditional VDB\n",
    "chroma_collection = chroma_client.get_collection(\"ai_arxiv_full\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence window VDB\n",
    "chroma_collection_sentence_window = chroma_client.get_collection(\"ai_arxiv_sentence_window\")\n",
    "vector_store_sentence_window = ChromaVectorStore(chroma_collection=chroma_collection_sentence_window)\n",
    "index_sentence_window = VectorStoreIndex.from_vector_store(vector_store=vector_store_sentence_window)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document summary VDB\n",
    "from llama_index.core import load_index_from_storage\n",
    "from llama_index.core import StorageContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storage_context = StorageContext.from_defaults(persist_dir=\"Obelix\")\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"ai_arxiv_doc_summary\")\n",
    "doc_summary_index = load_index_from_storage(llm=llm,\n",
    "                                              storage_context=storage_context,\n",
    "                                             embed_model = embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template\n",
    "with open(\"resources/text_qa_template.txt\", 'r', encoding='utf-8') as file:\n",
    "    text_qa_template_str = file.read()\n",
    "\n",
    "text_qa_template = PromptTemplate(text_qa_template_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tonic Validate setup\n",
    "benchmark = validate_api.get_benchmark(tonic_validate_benchmark_key)\n",
    "scorer = ValidateScorer(metrics=[RetrievalPrecisionMetric(), AnswerSimilarityMetric()],\n",
    "                        model_evaluator=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define query engines -------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive RAG\n",
    "query_engine_naive = index.as_query_engine(llm = llm,\n",
    "                                           text_qa_template=text_qa_template,\n",
    "                                           similarity_top_k=3,\n",
    "                                           embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cohere Rerank\n",
    "cohere_rerank = CohereRerank(api_key=config['cohere_api_key'], top_n=3)  # Ensure top_n matches k in naive RAG for comparability\n",
    "query_engine_rerank = index.as_query_engine(\n",
    "    similarity_top_k=10,\n",
    "    text_qa_template=text_qa_template,\n",
    "    node_postprocessors=[cohere_rerank],\n",
    "    llm = llm,\n",
    "    embed_model=embed_model\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ARAGOG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
