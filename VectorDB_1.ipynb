{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do Vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index\n",
    "# !pip install chromadb\n",
    "# !pip install --upgrade llama-index\n",
    "# !pip install --upgrade llama-index llama-index-vector-stores-chroma\n",
    "\n",
    "# from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries for loading datasets, data manipulation, document processing, vector storage, and embeddings.\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from llama_index.core import Document, StorageContext, VectorStoreIndex\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
    "import chromadb\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "from utils import chunked_iterable, load_config\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "import openai\n",
    "import os\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\Hassan\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# Hardcoded values for easy adjustment\n",
    "CHUNK_SIZE = 1000 #only for db upload\n",
    "TOKEN_CHUNK_SIZE = 512\n",
    "CHUNK_OVERLAP = 50\n",
    "\n",
    "# Load the config file\n",
    "load_config()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_key\n",
    "hf_read_token = os.getenv(\"HF_READ_TOKEN\")\n",
    "from huggingface_hub import login\n",
    "login(hf_read_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get hugging face token and log in.\n",
    "# # load_dotenv()\n",
    "# load_config()\n",
    "# hf_read_token = os.getenv(\"HF_READ_TOKEN\")\n",
    "# # print(f\"Token is: {hf_read_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hardcoded values for easy adjustment\n",
    "CHUNK_SIZE = 1000 #only for db upload\n",
    "TOKEN_CHUNK_SIZE = 512\n",
    "CHUNK_OVERLAP = 50\n",
    "\n",
    "# Load the config file\n",
    "load_config()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "# !huggingface-cli login --add-to-git-credential\n",
    "# # Load dataset and convert to DataFrame for easier manipulation\n",
    "# dataset = load_dataset(\"jamescalam/ai-arxiv\")\n",
    "# # dataset = Dataset.load_from_disk(\"dataset1/aragog_data/ai_aragog1\")\n",
    "# df = pd.DataFrame(dataset['train'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid pattern: '**' can only be an entire path component",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 15\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# from datasets import get_dataset_config_names\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# # This will show the cache location\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# df_dataset = pd.DataFrame(dataset)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# df_dataset.__len__\u001b[39;00m\n\u001b[0;32m     14\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuggingface-cli login --add-to-git-credential\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m dataset2 \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjamescalam/ai-arxiv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Hassan\\anaconda3\\envs\\ARAGOG\\lib\\site-packages\\datasets\\load.py:1759\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, **config_kwargs)\u001b[0m\n\u001b[0;32m   1754\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[0;32m   1755\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[0;32m   1756\u001b[0m )\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[1;32m-> 1759\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m load_dataset_builder(\n\u001b[0;32m   1760\u001b[0m     path\u001b[38;5;241m=\u001b[39mpath,\n\u001b[0;32m   1761\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[0;32m   1762\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n\u001b[0;32m   1763\u001b[0m     data_files\u001b[38;5;241m=\u001b[39mdata_files,\n\u001b[0;32m   1764\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   1765\u001b[0m     features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[0;32m   1766\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[0;32m   1767\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[0;32m   1768\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m   1769\u001b[0m     use_auth_token\u001b[38;5;241m=\u001b[39muse_auth_token,\n\u001b[0;32m   1770\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_kwargs,\n\u001b[0;32m   1771\u001b[0m )\n\u001b[0;32m   1773\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[1;32mc:\\Users\\Hassan\\anaconda3\\envs\\ARAGOG\\lib\\site-packages\\datasets\\load.py:1496\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[1;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, **config_kwargs)\u001b[0m\n\u001b[0;32m   1494\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[0;32m   1495\u001b[0m     download_config\u001b[38;5;241m.\u001b[39muse_auth_token \u001b[38;5;241m=\u001b[39m use_auth_token\n\u001b[1;32m-> 1496\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1499\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1500\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m builder_cls \u001b[38;5;241m=\u001b[39m import_main_class(dataset_module\u001b[38;5;241m.\u001b[39mmodule_path)\n",
      "File \u001b[1;32mc:\\Users\\Hassan\\anaconda3\\envs\\ARAGOG\\lib\\site-packages\\datasets\\load.py:1218\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[1;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[0;32m   1213\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[0;32m   1214\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m   1215\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a dataset script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or any data file in the same directory. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1216\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1217\u001b[0m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1218\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1219\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1220\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m   1221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a dataset script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or any data file in the same directory.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1222\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Hassan\\anaconda3\\envs\\ARAGOG\\lib\\site-packages\\datasets\\load.py:1195\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[1;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[0;32m   1187\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m HubDatasetModuleFactoryWithScript(\n\u001b[0;32m   1188\u001b[0m             path,\n\u001b[0;32m   1189\u001b[0m             revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1192\u001b[0m             dynamic_modules_path\u001b[38;5;241m=\u001b[39mdynamic_modules_path,\n\u001b[0;32m   1193\u001b[0m         )\u001b[38;5;241m.\u001b[39mget_module()\n\u001b[0;32m   1194\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mHubDatasetModuleFactoryWithoutScript\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1196\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1197\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1198\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1199\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1200\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1201\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1202\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1203\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[0;32m   1204\u001b[0m     \u001b[38;5;167;01mException\u001b[39;00m\n\u001b[0;32m   1205\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m e1:  \u001b[38;5;66;03m# noqa: all the attempts failed, before raising the error we should check if the module is already cached.\u001b[39;00m\n\u001b[0;32m   1206\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Hassan\\anaconda3\\envs\\ARAGOG\\lib\\site-packages\\datasets\\load.py:767\u001b[0m, in \u001b[0;36mHubDatasetModuleFactoryWithoutScript.get_module\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_module\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DatasetModule:\n\u001b[0;32m    757\u001b[0m     hfh_dataset_info \u001b[38;5;241m=\u001b[39m hf_api_dataset_info(\n\u001b[0;32m    758\u001b[0m         HfApi(config\u001b[38;5;241m.\u001b[39mHF_ENDPOINT),\n\u001b[0;32m    759\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    762\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100.0\u001b[39m,\n\u001b[0;32m    763\u001b[0m     )\n\u001b[0;32m    764\u001b[0m     patterns \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    765\u001b[0m         sanitize_patterns(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_files)\n\u001b[0;32m    766\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_files \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 767\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mget_data_patterns_in_dataset_repository\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhfh_dataset_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    768\u001b[0m     )\n\u001b[0;32m    769\u001b[0m     data_files \u001b[38;5;241m=\u001b[39m DataFilesDict\u001b[38;5;241m.\u001b[39mfrom_hf_repo(\n\u001b[0;32m    770\u001b[0m         patterns,\n\u001b[0;32m    771\u001b[0m         dataset_info\u001b[38;5;241m=\u001b[39mhfh_dataset_info,\n\u001b[0;32m    772\u001b[0m         base_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir,\n\u001b[0;32m    773\u001b[0m         allowed_extensions\u001b[38;5;241m=\u001b[39mALL_ALLOWED_EXTENSIONS,\n\u001b[0;32m    774\u001b[0m     )\n\u001b[0;32m    775\u001b[0m     module_names \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    776\u001b[0m         key: infer_module_for_data_files(data_files_list, use_auth_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39muse_auth_token)\n\u001b[0;32m    777\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key, data_files_list \u001b[38;5;129;01min\u001b[39;00m data_files\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    778\u001b[0m     }\n",
      "File \u001b[1;32mc:\\Users\\Hassan\\anaconda3\\envs\\ARAGOG\\lib\\site-packages\\datasets\\data_files.py:675\u001b[0m, in \u001b[0;36mget_data_patterns_in_dataset_repository\u001b[1;34m(dataset_info, base_path)\u001b[0m\n\u001b[0;32m    673\u001b[0m resolver \u001b[38;5;241m=\u001b[39m partial(_resolve_single_pattern_in_dataset_repository, dataset_info, base_path\u001b[38;5;241m=\u001b[39mbase_path)\n\u001b[0;32m    674\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 675\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_data_files_patterns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresolver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[0;32m    677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m EmptyDatasetError(\n\u001b[0;32m    678\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe dataset repository at \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_info\u001b[38;5;241m.\u001b[39mid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt contain any data files\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    679\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Hassan\\anaconda3\\envs\\ARAGOG\\lib\\site-packages\\datasets\\data_files.py:236\u001b[0m, in \u001b[0;36m_get_data_files_patterns\u001b[1;34m(pattern_resolver)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m patterns:\n\u001b[1;32m--> 236\u001b[0m         data_files \u001b[38;5;241m=\u001b[39m \u001b[43mpattern_resolver\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data_files) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    238\u001b[0m             non_empty_splits\u001b[38;5;241m.\u001b[39mappend(split)\n",
      "File \u001b[1;32mc:\\Users\\Hassan\\anaconda3\\envs\\ARAGOG\\lib\\site-packages\\datasets\\data_files.py:486\u001b[0m, in \u001b[0;36m_resolve_single_pattern_in_dataset_repository\u001b[1;34m(dataset_info, pattern, base_path, allowed_extensions)\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    485\u001b[0m     base_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 486\u001b[0m glob_iter \u001b[38;5;241m=\u001b[39m [PurePath(filepath) \u001b[38;5;28;01mfor\u001b[39;00m filepath \u001b[38;5;129;01min\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPurePath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_posix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m fs\u001b[38;5;241m.\u001b[39misfile(filepath)]\n\u001b[0;32m    487\u001b[0m matched_paths \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    488\u001b[0m     filepath\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m filepath \u001b[38;5;129;01min\u001b[39;00m glob_iter\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    496\u001b[0m     )\n\u001b[0;32m    497\u001b[0m ]  \u001b[38;5;66;03m# ignore .ipynb and __pycache__, but keep /../\u001b[39;00m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allowed_extensions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Hassan\\anaconda3\\envs\\ARAGOG\\lib\\site-packages\\fsspec\\spec.py:613\u001b[0m, in \u001b[0;36mAbstractFileSystem.glob\u001b[1;34m(self, path, maxdepth, **kwargs)\u001b[0m\n\u001b[0;32m    609\u001b[0m         depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    611\u001b[0m allpaths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfind(root, maxdepth\u001b[38;5;241m=\u001b[39mdepth, withdirs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, detail\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 613\u001b[0m pattern \u001b[38;5;241m=\u001b[39m \u001b[43mglob_translate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mends_with_sep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    614\u001b[0m pattern \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mcompile(pattern)\n\u001b[0;32m    616\u001b[0m out \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    617\u001b[0m     p: info\n\u001b[0;32m    618\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p, info \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(allpaths\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    625\u001b[0m     )\n\u001b[0;32m    626\u001b[0m }\n",
      "File \u001b[1;32mc:\\Users\\Hassan\\anaconda3\\envs\\ARAGOG\\lib\\site-packages\\fsspec\\utils.py:729\u001b[0m, in \u001b[0;36mglob_translate\u001b[1;34m(pat)\u001b[0m\n\u001b[0;32m    727\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m**\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m part:\n\u001b[1;32m--> 729\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    730\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid pattern: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m**\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m can only be an entire path component\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    731\u001b[0m     )\n\u001b[0;32m    732\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m part:\n\u001b[0;32m    733\u001b[0m     results\u001b[38;5;241m.\u001b[39mextend(_translate(part, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_sep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m, not_sep))\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid pattern: '**' can only be an entire path component"
     ]
    }
   ],
   "source": [
    "# from datasets import get_dataset_config_names\n",
    "\n",
    "# # This will show the cache location\n",
    "# from datasets.utils.logging import get_verbosity\n",
    "# print(f\"Cache directory: {get_verbosity()}\")\n",
    "# from datasets import config\n",
    "\n",
    "# # Display the cache directory\n",
    "# print(f\"Cache directory: {config.HF_DATASETS_CACHE}\")\n",
    "# dataset.column_names\n",
    "# dataset.save_to_disk('dataset1')\n",
    "# df_dataset = pd.DataFrame(dataset)\n",
    "# df_dataset.__len__\n",
    "# !huggingface-cli login --add-to-git-credential\n",
    "# dataset2 = load_dataset(\"jamescalam/ai-arxiv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/Hassan/AppData/Local/Temp/tmpfzi9l8d8/dataset1/state.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_from_disk\n\u001b[1;32m----> 2\u001b[0m dataset1 \u001b[38;5;241m=\u001b[39m \u001b[43mload_from_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdataset1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Hassan\\anaconda3\\envs\\ARAGOG\\lib\\site-packages\\datasets\\load.py:1872\u001b[0m, in \u001b[0;36mload_from_disk\u001b[1;34m(dataset_path, fs, keep_in_memory, storage_options)\u001b[0m\n\u001b[0;32m   1868\u001b[0m data_files \u001b[38;5;241m=\u001b[39m builder_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_files\u001b[39m\u001b[38;5;124m\"\u001b[39m, data_files)\n\u001b[0;32m   1869\u001b[0m config_name \u001b[38;5;241m=\u001b[39m builder_kwargs\u001b[38;5;241m.\u001b[39mpop(\n\u001b[0;32m   1870\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, name \u001b[38;5;129;01mor\u001b[39;00m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_configs_parameters\u001b[38;5;241m.\u001b[39mdefault_config_name\n\u001b[0;32m   1871\u001b[0m )\n\u001b[1;32m-> 1872\u001b[0m dataset_name \u001b[38;5;241m=\u001b[39m builder_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   1873\u001b[0m info \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mdataset_infos\u001b[38;5;241m.\u001b[39mget(config_name) \u001b[38;5;28;01mif\u001b[39;00m dataset_module\u001b[38;5;241m.\u001b[39mdataset_infos \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1876\u001b[0m     path \u001b[38;5;129;01min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[0;32m   1877\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m data_files \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_configs_parameters\u001b[38;5;241m.\u001b[39mbuilder_configs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdata_files \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1879\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\Hassan\\anaconda3\\envs\\ARAGOG\\lib\\site-packages\\datasets\\arrow_dataset.py:1558\u001b[0m, in \u001b[0;36mload_from_disk\u001b[1;34m(dataset_path, fs, keep_in_memory, storage_options)\u001b[0m\n\u001b[0;32m   1553\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(state, state_file, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, sort_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1554\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mopen(\n\u001b[0;32m   1555\u001b[0m     posixpath\u001b[38;5;241m.\u001b[39mjoin(dataset_path, config\u001b[38;5;241m.\u001b[39mDATASET_INFO_FILENAME), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1556\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m dataset_info_file:\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;66;03m# Sort only the first level of keys, or we might shuffle fields of nested features if we use sort_keys=True\u001b[39;00m\n\u001b[1;32m-> 1558\u001b[0m     sorted_keys_dataset_info \u001b[38;5;241m=\u001b[39m {key: dataset_info[key] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(dataset_info)}\n\u001b[0;32m   1559\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(sorted_keys_dataset_info, dataset_info_file, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/Hassan/AppData/Local/Temp/tmpfzi9l8d8/dataset1/state.json'"
     ]
    }
   ],
   "source": [
    "# do not use\n",
    "# from datasets import load_from_disk\n",
    "# dataset1 = load_from_disk('dataset1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read when getting dataset as pandas and storing locally\n",
    "# instead of reading dataset, just read the pandas and save it because that is what is used\n",
    "# import pandas as pd\n",
    "\n",
    "# df_archiv = pd.read_json(\"hf://datasets/jamescalam/ai-arxiv/train.jsonl\", lines=True)\n",
    "# data_path = './dataset1/archiv_data.csv'\n",
    "# df_archiv.to_csv(data_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'title', 'summary', 'source', 'authors', 'categories', 'comment',\n",
       "       'journal_ref', 'primary_category', 'published', 'updated', 'content',\n",
       "       'references'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for debugging\n",
    "# data_path = './dataset1/archiv_data.csv'\n",
    "# df_archiv_local = pd.read_csv(data_path)\n",
    "# df_archiv_local.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare document objects from the dataset for indexing\n",
    "# documents = [Document(text=content) for content in df['content']]\n",
    "documents = [Document(text=content) for content in df_archiv_local['content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the embedding model\n",
    "embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\")\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classic vector DB\n",
    "# Initialize a text splitter with hardcoded values for chunking documents\n",
    "parser = TokenTextSplitter(chunk_size=TOKEN_CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "\n",
    "\n",
    "# # chroma_collection = chroma_client.create_collection(\"ai_arxiv_full\")\n",
    "# chroma_collection = chroma_client.get_collection(\"ai_arxiv_full\")\n",
    "\n",
    "# vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "# storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# index = VectorStoreIndex(\n",
    "#     nodes, storage_context=storage_context,\n",
    "#     embed_model=embed_model,\n",
    "#     use_async=True\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Continue with your existing code\n",
    "chroma_collection = chroma_client.get_collection(\"ai_arxiv_full\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex(\n",
    "    nodes,\n",
    "    storage_context=storage_context,\n",
    "    embed_model=embed_model,\n",
    "    use_async=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence window\n",
    "node_parser_sentence_window = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=3,\n",
    "    window_metadata_key=\"window\",\n",
    "    original_text_metadata_key=\"original_text\",\n",
    ")\n",
    "nodes_sentence_window = node_parser_sentence_window.get_nodes_from_documents(documents)\n",
    "nodes_sentence_window_filtered = filter_large_nodes(nodes_sentence_window)\n",
    "\n",
    "# chroma_collection_sentence_window = chroma_client.create_collection(\"ai_arxiv_sentence_window\")\n",
    "\n",
    "chroma_collection_sentence_window = chroma_client.create_collection(\n",
    "    \"ai_arxiv_sentence_window\", get_or_create=True\n",
    ")\n",
    "\n",
    "vector_store_sentence_window = ChromaVectorStore(chroma_collection=chroma_collection_sentence_window)\n",
    "\n",
    "storage_context_sentence_window = StorageContext.from_defaults(vector_store=vector_store_sentence_window)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 21/21 [00:03<00:00,  6.67it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:03<00:00,  5.66it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:02<00:00,  8.68it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:02<00:00,  8.79it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:04<00:00,  5.18it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:04<00:00,  5.16it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:05<00:00,  3.80it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:11<00:00,  1.79it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:02<00:00,  7.34it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:02<00:00,  7.74it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:04<00:00,  4.61it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:05<00:00,  3.52it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:02<00:00,  7.10it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:05<00:00,  3.57it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:05<00:00,  3.53it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:04<00:00,  4.30it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:04<00:00,  4.81it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:05<00:00,  3.90it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:04<00:00,  4.29it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:05<00:00,  3.66it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:04<00:00,  4.42it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:02<00:00,  8.62it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:03<00:00,  5.61it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:03<00:00,  5.64it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:03<00:00,  6.09it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:03<00:00,  5.81it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:04<00:00,  4.89it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:04<00:00,  4.47it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:02<00:00,  7.68it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:03<00:00,  6.59it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:03<00:00,  6.60it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:02<00:00,  7.27it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:02<00:00,  7.38it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:03<00:00,  6.84it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:05<00:00,  4.03it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:03<00:00,  6.85it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:04<00:00,  4.67it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:03<00:00,  6.60it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:05<00:00,  4.03it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:03<00:00,  5.54it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:03<00:00,  6.77it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:02<00:00,  8.41it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:03<00:00,  6.66it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:03<00:00,  6.35it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:02<00:00,  8.01it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:02<00:00,  7.02it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:03<00:00,  6.65it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:03<00:00,  6.22it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:02<00:00,  7.20it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:02<00:00,  8.51it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:03<00:00,  6.81it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:02<00:00,  7.57it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:03<00:00,  5.99it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:02<00:00, 10.46it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:02<00:00,  9.22it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:02<00:00,  7.80it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:03<00:00,  6.15it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:02<00:00,  7.09it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:03<00:00,  6.67it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:02<00:00,  7.55it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:02<00:00,  7.78it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:02<00:00,  8.25it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:03<00:00,  6.17it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:02<00:00,  7.56it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:26<00:00,  1.26s/it]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:04<00:00,  4.38it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:03<00:00,  6.77it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:02<00:00,  8.19it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:03<00:00,  6.06it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [01:06<00:00,  3.15s/it]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:09<00:00,  2.25it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:05<00:00,  3.55it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:06<00:00,  3.43it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:03<00:00,  5.67it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:02<00:00,  7.99it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:03<00:00,  5.49it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:03<00:00,  5.90it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:06<00:00,  3.25it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:03<00:00,  6.11it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:02<00:00,  7.54it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:02<00:00,  7.52it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:03<00:00,  5.68it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:03<00:00,  6.22it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:02<00:00,  7.55it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:02<00:00,  7.48it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:05<00:00,  3.92it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:03<00:00,  5.83it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:03<00:00,  5.93it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:03<00:00,  6.35it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:02<00:00,  9.13it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:02<00:00,  7.46it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:04<00:00,  5.06it/s]\n",
      "Generating embeddings: 100%|██████████| 20/20 [00:03<00:00,  5.13it/s]\n"
     ]
    }
   ],
   "source": [
    "# Removed nodes longer than 8K, they were causing issues and these are only tables and references which are not relevant,\n",
    "# in total 17 chunks or something\n",
    "index = VectorStoreIndex(\n",
    "    nodes_sentence_window_filtered,\n",
    "    storage_context=storage_context_sentence_window,\n",
    "    embed_model=embed_model,\n",
    "    use_async=True,\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document summary index\n",
    "# LLM (gpt-3.5-turbo)\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import DocumentSummaryIndex, get_response_synthesizer\n",
    "\n",
    "parser = TokenTextSplitter(chunk_size=3072, chunk_overlap=100)\n",
    "splits_doc_summary = parser.get_nodes_from_documents(documents)\n",
    "\n",
    "docs_for_summary = [Document(text=node.text, metadata=node.metadata) for node in splits_doc_summary]\n",
    "\n",
    "# from llama_index.llms.mistralai import MistralAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !pip show llama-index\n",
    "# # !pip install llama-index-llms-mistral\n",
    "# # !pip install llama-index-llm-mistral\n",
    "# !pip install llama-index mistralai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'MistralAI' from 'llama_index.llms' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[89], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MistralAI\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'MistralAI' from 'llama_index.llms' (unknown location)"
     ]
    }
   ],
   "source": [
    "from llama_index.llms import MistralAI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ARAGOG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
