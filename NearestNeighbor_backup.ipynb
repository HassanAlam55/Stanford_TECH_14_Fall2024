{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Guide To Approximate Nearest Neighbors Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exhaustive Search Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex\n",
    "import nmslib\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# does not work/url does not exit\n",
    "# import pickle\n",
    "# import faiss\n",
    "# def load_data():\n",
    "#     with open('movies.pickle', 'rb') as f:\n",
    "#         data = pickle.load(f)\n",
    "#     return data\n",
    "#     # return datadata = load_data()\n",
    "# data = load_data()\n",
    "# print (data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## recrateing movie lesne directly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess the Movie Data\n",
    "You need to prepare your movie data for encoding. For example, you may want to combine the title, description, and genres to form a rich text representation of each movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   movie_id              title release_date  video_release_date  \\\n",
      "0         1   Toy Story (1995)  01-Jan-1995                 NaN   \n",
      "1         2   GoldenEye (1995)  01-Jan-1995                 NaN   \n",
      "2         3  Four Rooms (1995)  01-Jan-1995                 NaN   \n",
      "3         4  Get Shorty (1995)  01-Jan-1995                 NaN   \n",
      "4         5     Copycat (1995)  01-Jan-1995                 NaN   \n",
      "\n",
      "                                            IMDb_URL  genre_0  genre_1  \\\n",
      "0  http://us.imdb.com/M/title-exact?Toy%20Story%2...        0        0   \n",
      "1  http://us.imdb.com/M/title-exact?GoldenEye%20(...        0        1   \n",
      "2  http://us.imdb.com/M/title-exact?Four%20Rooms%...        0        0   \n",
      "3  http://us.imdb.com/M/title-exact?Get%20Shorty%...        0        1   \n",
      "4  http://us.imdb.com/M/title-exact?Copycat%20(1995)        0        0   \n",
      "\n",
      "   genre_2  genre_3  genre_4  ...  genre_9  genre_10  genre_11  genre_12  \\\n",
      "0        0        1        1  ...        0         0         0         0   \n",
      "1        1        0        0  ...        0         0         0         0   \n",
      "2        0        0        0  ...        0         0         0         0   \n",
      "3        0        0        0  ...        0         0         0         0   \n",
      "4        0        0        0  ...        0         0         0         0   \n",
      "\n",
      "   genre_13  genre_14  genre_15  genre_16  genre_17  genre_18  \n",
      "0         0         0         0         0         0         0  \n",
      "1         0         0         0         1         0         0  \n",
      "2         0         0         0         1         0         0  \n",
      "3         0         0         0         0         0         0  \n",
      "4         0         0         0         1         0         0  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# Path to the movie metadata file\n",
    "# movies_file_path = 'ml-100k/u.item'\n",
    "movies_file_path = Path('./MovieLens/ml-100k/u.item')\n",
    "# Define all 24 column names based on the dataset description\n",
    "movie_columns = ['movie_id', 'title', 'release_date', 'video_release_date', 'IMDb_URL'] + \\\n",
    "                [f'genre_{i}' for i in range(19)]  # 19 genre columns\n",
    "\n",
    "\n",
    "\n",
    "# Load the data with the correct number of columns\n",
    "movies = pd.read_csv(movies_file_path, sep='|', encoding='latin-1', header=None, names=movie_columns)\n",
    "\n",
    "# Display the first few rows\n",
    "print(movies.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  movie_id  rating  timestamp\n",
      "0      196       242       3  881250949\n",
      "1      186       302       3  891717742\n",
      "2       22       377       1  878887116\n",
      "3      244        51       2  880606923\n",
      "4      166       346       1  886397596\n",
      "MovieLens 100K ratings data saved to movielens_100k_ratings.pickle!\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import pickle\n",
    "\n",
    "# Load the MovieLens 100K ratings data\n",
    "# ratings_file_path = 'ml-100k/u.data'  # Replace with the correct path to the 'u.data' file\n",
    "ratings_file_path = Path('./MovieLens/ml-100k/u.data')\n",
    "\n",
    "# The data does not have headers in the file, so we specify them\n",
    "columns = ['user_id', 'movie_id', 'rating', 'timestamp']\n",
    "ratings = pd.read_csv(ratings_file_path, sep='\\t', names=columns)\n",
    "\n",
    "# Display first few rows of the dataset\n",
    "print(ratings.head())\n",
    "\n",
    "# Save the ratings data as a pickle file\n",
    "with open('movielens_100k_ratings.pickle', 'wb') as f:\n",
    "    pickle.dump(ratings, f)\n",
    "\n",
    "print(\"MovieLens 100K ratings data saved to movielens_100k_ratings.pickle!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   movie_id            text_representation\n",
      "0         1   Toy Story (1995) 01-Jan-1995\n",
      "1         2   GoldenEye (1995) 01-Jan-1995\n",
      "2         3  Four Rooms (1995) 01-Jan-1995\n",
      "3         4  Get Shorty (1995) 01-Jan-1995\n",
      "4         5     Copycat (1995) 01-Jan-1995\n"
     ]
    }
   ],
   "source": [
    "# Example: Create a text representation for each movie by combining title and genres\n",
    "movies['text_representation'] = movies['title'] + ' ' + movies['release_date']\n",
    "\n",
    "# Display the dataset with the text representation\n",
    "print(movies[['movie_id', 'text_representation']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a Pretrained Model to Generate Embeddings\n",
    "Now, weâ€™ll use a pretrained transformer model to create semantic embeddings for the movie text data. The model will convert the text representation of each movie into a vector of fixed size, capturing its semantic meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hassan\\anaconda3\\envs\\Py39Stanford1\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db75c14eead040fca48a08bc5df63591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hassan\\anaconda3\\envs\\Py39Stanford1\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Hassan\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7a667cc63534c65a93ac5a23bec3b44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d15214a89dd14de09454777cf3523a44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c19de44687f4093ba1098ce10603412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bde22eeb9f3841bdaff4385b872bf33c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ec3696178e4dc0ae4c3c906ad47037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49b4ffe23056402c9a11dc7b4f582c17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a890745a687a46c9a3fe848aa9fc12d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e695ada4dc644a1b3c9b1a33aea8067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e2e2aa73833460e823eaee7815f5bd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "704c6d3c80204859b33c3b0c67fb75c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec5175fcec3043ca8b6ab1518926c4f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hassan\\anaconda3\\envs\\Py39Stanford1\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embeddings: (1682, 384)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a pre-trained model (e.g., 'all-MiniLM-L6-v2' is small and fast)\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate semantic embeddings for the movie text representations\n",
    "movie_descriptions = movies['text_representation'].tolist()\n",
    "movie_embeddings = model.encode(movie_descriptions, show_progress_bar=True)\n",
    "\n",
    "# The movie_embeddings is a matrix where each row is the vector for a movie\n",
    "print(f\"Shape of embeddings: {movie_embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Encoded Data\n",
    "You can now save these embeddings for future use (e.g., for similarity search or as input to a recommendation model). You might also want to save the movie titles along with the embeddings to keep track of which vector corresponds to which movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie embeddings saved to 'movie_embeddings.pickle'\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the movie embeddings and their corresponding titles\n",
    "with open('movie_embeddings.pickle', 'wb') as f:\n",
    "    pickle.dump((movies['movie_id'].tolist(), movie_embeddings), f)\n",
    "\n",
    "print(\"Movie embeddings saved to 'movie_embeddings.pickle'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Similarity Search (Optional)\n",
    "If your goal is to perform a similarity search (e.g., find similar movies based on their semantic embeddings), you can use FAISS, a library that allows efficient similarity searches over large vector spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie ID: 1, Title: Toy Story (1995)\n",
      "Movie ID: 1219, Title: Goofy Movie, A (1995)\n",
      "Movie ID: 1470, Title: Gumby: The Movie (1995)\n",
      "Movie ID: 772, Title: Kids (1995)\n",
      "Movie ID: 93, Title: Welcome to the Dollhouse (1995)\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Convert embeddings to a NumPy array\n",
    "movie_embeddings_np = np.array(movie_embeddings)\n",
    "\n",
    "# Create a FAISS index\n",
    "index = faiss.IndexFlatL2(movie_embeddings_np.shape[1])  # L2 distance (Euclidean)\n",
    "index.add(movie_embeddings_np)  # Add embeddings to the index\n",
    "\n",
    "# Search for the 5 most similar movies to the first movie in the dataset\n",
    "query_embedding = movie_embeddings_np[0].reshape(1, -1)\n",
    "distances, indices = index.search(query_embedding, 5)\n",
    "\n",
    "# Print the most similar movies\n",
    "for idx in indices[0]:\n",
    "    print(f\"Movie ID: {movies.iloc[idx]['movie_id']}, Title: {movies.iloc[idx]['title']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_representation_array = np.array(movies['text_representation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### copy paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExactIndex():\n",
    "    def __init__(self, vectors, labels):\n",
    "        self.dimension = vectors.shape[1]\n",
    "        self.vectors = vectors.astype('float32')\n",
    "        # self.lab\n",
    "        self.labels = labels\n",
    "   \n",
    "    def build(self):\n",
    "        self.index = faiss.IndexFlatL2(self.dimension,)\n",
    "        self.index.add(self.vectors)\n",
    "        \n",
    "    def query(self, vectors, k=10):\n",
    "        distances, indices = self.index.search(vectors, k) \n",
    "        # I expect only query on one vector thus the slice\n",
    "        return [self.labels[i] for i in indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build with IndexFlatL2 0.0039975643157958984\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "movie_desc = np.array(movies['text_representation'])\n",
    "index = ExactIndex(movie_embeddings_np, movie_desc)\n",
    "index.build()\n",
    "end_time = time.time()\n",
    "print(f'Time to build with IndexFlatL2 {end_time - start_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to query with IndexFlatL2 0.000997781753540039\n"
     ]
    }
   ],
   "source": [
    "# index.query(data['vector'][0])\n",
    "start_time = time.time()\n",
    "index.query(movie_embeddings_np[0].reshape(1, -1))\n",
    "# movie_embeddings_np[0]\n",
    "end_time = time.time()\n",
    "print(f'Time to query with IndexFlatL2 {end_time - start_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN implementaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # class AnnoyIndex():\n",
    "# #     def __init__(self, vectors, labels):\n",
    "# #         self.dimension = vectors.shape[1]\n",
    "# #         self.vectors = vectors.astype('float32')\n",
    "# #         self.labels = labels    \n",
    "   \n",
    "# #     def build(self, number_of_trees=5):\n",
    "# #         self.index = annoy.AnnoyIndex(self.dimension)\n",
    "# #         for i, vec in enumerate(self.vectors):\n",
    "# #             self.index.add_item(i, vec.tolist())\n",
    "# #         self.index.build(number_of_trees)\n",
    "        \n",
    "# #     def query(self, vector, k=10):\n",
    "# #         indices = self.index.get_nns_by_vector(\n",
    "# #               vector.tolist(), \n",
    "# #               k, \n",
    "# #               search_k=search_in_x_trees)                                           \n",
    "# #         return [self.labels[i] for i in indices]\n",
    "\n",
    "# from annoy import AnnoyIndex\n",
    "\n",
    "# class AnnoyIndexWrapper():\n",
    "#     def __init__(self, vectors, labels):\n",
    "#         self.dimension = vectors.shape[1]\n",
    "#         self.vectors = vectors.astype('float32')\n",
    "#         self.labels = labels    \n",
    "   \n",
    "#     def build(self, number_of_trees=5):\n",
    "#         self.index = AnnoyIndex(self.dimension)\n",
    "#         for i, vec in enumerate(self.vectors):\n",
    "#             self.index.add_item(i, vec.tolist())\n",
    "#         self.index.build(number_of_trees)\n",
    "        \n",
    "#     def query(self, vector, k=10):\n",
    "#         indices = self.index.get_nns_by_vector(\n",
    "#             vector.tolist(), \n",
    "#             k, \n",
    "#             search_k=search_in_x_trees\n",
    "#         )                                           \n",
    "#         return [self.labels[i] for i in indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ann_index = AnnoyIndex(movie_embeddings_np, movie_desc)\n",
    "# ann_index.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnnoyIndexWrapper():\n",
    "    def __init__(self, vectors, labels):\n",
    "        self.dimension = vectors.shape[1]\n",
    "        self.vectors = vectors.astype('float32')\n",
    "        self.labels = labels\n",
    "    \n",
    "    def build(self, number_of_trees=5):\n",
    "        self.index = AnnoyIndex(self.dimension, 'angular')  # Added metric type\n",
    "        for i, vec in enumerate(self.vectors):\n",
    "            self.index.add_item(i, vec.tolist())\n",
    "        self.index.build(number_of_trees)\n",
    "    \n",
    "    def query(self, vector, k=10):\n",
    "        indices = self.index.get_nns_by_vector(\n",
    "            vector.tolist(), \n",
    "            k, \n",
    "            search_k=-1\n",
    "        )\n",
    "        return [self.labels[i] for i in indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build with AnnoyIndex 0.05190420150756836\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "ann_index = AnnoyIndexWrapper(movie_embeddings_np, movie_desc)\n",
    "ann_index.build()\n",
    "end_time = time.time()\n",
    "print(f'Time to build with AnnoyIndex {end_time - start_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to query with AnnoyIndex 0.000989675521850586\n"
     ]
    }
   ],
   "source": [
    "# index.query(data['vector'][0])\n",
    "# ann_index.query(movie_embeddings_np[0].reshape(1, -1))\n",
    "start_time = time.time()\n",
    "query_vector = movie_embeddings_np[0].reshape(-1) \n",
    "ann_index.query(query_vector)\n",
    "end_time = time.time()\n",
    "print(f'Time to query with AnnoyIndex {end_time - start_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Encoding Using LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSHIndex():\n",
    "    def __init__(self, vectors, labels):\n",
    "        self.dimension = vectors.shape[1]\n",
    "        self.vectors = vectors.astype('float32')\n",
    "        self.labels = labels    \n",
    "   \n",
    "    def build(self, num_bits=8):\n",
    "        self.index = faiss.IndexLSH(self.dimension, num_bits)\n",
    "        self.index.add(self.vectors)\n",
    "        \n",
    "    def query(self, vectors, k=10):\n",
    "        distances, indices = self.index.search(vectors, k) \n",
    "        # I expect only query on one vector thus the slice\n",
    "        return [self.labels[i] for i in indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build with LSHIndex 0.0029926300048828125\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "index_lsh = LSHIndex(movie_embeddings_np, movie_desc)\n",
    "index_lsh.build()\n",
    "end_time = time.time()\n",
    "print(f'Time to build with LSHIndex {end_time - start_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to search with LSHIndex 0.0009503364562988281\n"
     ]
    }
   ],
   "source": [
    "# query_vector.reshape(1, -1)\n",
    "start_time = time.time()\n",
    "movie_embeddings_np[0]\n",
    "index_lsh.query(movie_embeddings_np[0].reshape(1, -1))\n",
    "end_time = time.time()\n",
    "print(f'Time to search with LSHIndex {end_time - start_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Encoding Using Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IVPQIndex():\n",
    "    def __init__(self, vectors, labels):\n",
    "        self.dimension = vectors.shape[1]\n",
    "        self.vectors = vectors.astype('float32')\n",
    "        self.labels = labels    \n",
    "    \n",
    "    def build(self, \n",
    "              number_of_partition=8, \n",
    "              search_in_x_partitions=2, \n",
    "              subvector_size=8):\n",
    "        quantizer = faiss.IndexFlatL2(self.dimension)\n",
    "        self.index = faiss.IndexIVFPQ(quantizer, \n",
    "                                      self.dimension, \n",
    "                                      number_of_partition, \n",
    "                                      search_in_x_partitions, \n",
    "                                      subvector_size)\n",
    "        self.index.train(self.vectors)\n",
    "        self.index.add(self.vectors)\n",
    "        \n",
    "    def query(self, vectors, k=10):\n",
    "        distances, indices = self.index.search(vectors, k) \n",
    "        # I expect only query on one vector thus the slice\n",
    "        return [self.labels[i] for i in indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build with Quant_index 0.1565852165222168\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "Quant_index = IVPQIndex(movie_embeddings_np, movie_desc)\n",
    "Quant_index.build()\n",
    "end_time = time.time()\n",
    "print(f'Time to build with Quant_index {end_time - start_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to search  with Quant_index 0.0009963512420654297\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "Quant_index.query(movie_embeddings_np[0].reshape(1, -1))\n",
    "end_time = time.time()\n",
    "print(f'Time to search  with Quant_index {end_time - start_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Navigable Small World Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMSLIBIndex():\n",
    "    def __init__(self, vectors, labels):\n",
    "        self.dimention = vectors.shape[1]\n",
    "        self.vectors = vectors.astype('float32')\n",
    "        self.labels = labels\n",
    "    def build(self):\n",
    "        self.index = nmslib.init(method='hnsw', space='cosinesimil')\n",
    "        self.index.addDataPointBatch(self.vectors)\n",
    "        self.index.createIndex({'post': 2})\n",
    "        \n",
    "    def query(self, vector, k=10):\n",
    "        indices = self.index.knnQuery(vector, k=k)\n",
    "        return [self.labels[i] for i in indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build with nmslib 0.14765191078186035\n"
     ]
    }
   ],
   "source": [
    "import nmslib\n",
    "movie_desc = np.array(movies['text_representation'])\n",
    "start_time = time.time()\n",
    "NSM_index = NMSLIBIndex(movie_embeddings_np, movie_desc)\n",
    "NSM_index.build()\n",
    "end_time = time.time()\n",
    "print(f'Time to build with nmslib {end_time - start_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search ['Toy Story (1995) 01-Jan-1995', 'Goofy Movie, A (1995) 01-Jan-1995', 'Gumby: The Movie (1995) 01-Jan-1995', 'Kids (1995) 01-Jan-1995', 'Welcome to the Dollhouse (1995) 24-May-1996', 'Little Princess, A (1995) 01-Jan-1995', 'Candyman: Farewell to the Flesh (1995) 01-Jan-1995', 'Castle Freak (1995) 01-Jan-1995', 'Boys Life (1995) 01-Jan-1995', 'Friday (1995) 01-Jan-1995']\n",
      "Time to search with nmslib 0.0009999275207519531\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print (f'search {NSM_index.query(movie_embeddings_np[0].reshape(1, -1))}')\n",
    "end_time = time.time()\n",
    "print(f'Time to search with nmslib {end_time - start_time}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py39Stanford1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
